{"id": "7ea223f3-ca9f-4488-b560-2ad2706c7bcb", "text": "Q: Floating point comparison in shell A: <p>Don't forget <a href=\"/questions/tagged/ksh\" class=\"s-tag post-tag\" title=\"show questions tagged &#39;ksh&#39;\" aria-label=\"show questions tagged &#39;ksh&#39;\" rel=\"tag\" aria-labelledby=\"tag-ksh-tooltip-container\" data-tag-menu-origin=\"Unknown\">ksh</a>, <a href=\"/questions/tagged/bash\" class=\"s-tag post-tag\" title=\"show questions tagged &#39;bash&#39;\" aria-label=\"show questions tagged &#39;bash&#39;\" rel=\"tag\" aria-labelledby=\"tag-bash-tooltip-container\" data-tag-menu-origin=\"Unknown\">bash</a> was largely inspired by <code>ksh</code> and they share many similarities.</p>\n<pre class=\"lang-bash prettyprint-override\"><code>ksh\n$ echo $((10,1/1,1))\n9,18181818181818182\n</code></pre>\n", "tags": ["bash", "shell"]}
{"id": "8bab4b69-fffd-4920-96d7-da41e0383074", "text": "Q: BullMQ: How to schedule and execute a job once at a specific timestamp in the future without using delay? A: <p>Good question \u2014 this trips up a lot of people when they start with delayed jobs.</p>\n<p>Why BullMQ uses delay instead of timestamps</p>\n<p>BullMQ (and most Redis-backed queues) use relative delays because Redis doesn't have native wall-clock scheduling. Under the hood, it does store timestamps, but it<br />\nwants you to calculate the delay first to avoid clock sync issues between your app servers and Redis.</p>\n<p>Using relative delays also prevents problems when:<br />\n\u2022 App servers and Redis have slightly different clocks<br />\n\u2022 Jobs are enqueued from different time zones<br />\n\u2022 System clocks drift or get adjusted</p>\n<p>The delay gets calculated once at enqueue time and stays consistent.</p>\n<p>Built-in timestamp scheduling?</p>\n<p>Nope, not directly. What you're doing is the standard approach:</p>\n<p>javascript</p>\n<pre><code>const delay = targetTime.getTime() - Date.now();  \nawait queue.add(name, data, { delay }); \n</code></pre>\n<p>Most teams wrap this in a helper to keep it clean:</p>\n<p>javascript</p>\n<pre><code>function addAt(queue, name, data, runAt) {  \n    return queue.add(name, data, { delay: runAt.getTime() - Date.now() });  \n}  \n</code></pre>\n<p>Worker setup</p>\n<p>You don't need to recreate workers. Once a worker is running, it continuously polls Redis and will automatically pick up delayed jobs when they become ready. As long<br />\nas at least one worker is running when the job should execute, you're good.</p>\n<p>Real-world gotchas</p>\n<p>A few things people run into:<br />\n\u2022 Very long delays (weeks/months) can be fragile if Redis restarts<br />\n\u2022 Limited visibility into why a delayed job hasn't fired yet<br />\n\u2022 Rescheduling jobs usually requires custom tooling</p>\n<p>For short delays, BullMQ works great. For complex scheduling or business-critical workflows, teams sometimes move to more explicit orchestration systems that handle<br />\nlong-term scheduling better.</p>\n", "tags": ["javascript", "node.js", "queue", "worker", "bullmq"]}
{"id": "e7ea2685-1152-445e-930b-769381f858aa", "text": "Q: SwiftUI .popover frame size incorrect A: <p>My pop-up view is more complicated than just a <code>Text</code> so the solution with <code>PreferenceKey</code> does not work for me, but I found a simpler one. The <code>@Binding</code> height works well like that:</p>\n<pre><code>struct InfoButton: View {    \n    @State private var height: CGFloat = 100 // &lt;- this \n    @State private var goNext = false\n    \n    var body: some View {\n        GeometryReader { geometry in\n            Button {\n                goNext.toggle()\n            } label: {\n                Image(systemName: &quot;info.circle&quot;)\n                    .imageScale(.large)\n            }\n            .frame(maxWidth: .infinity, maxHeight: .infinity)\n            .popover(isPresented: $goNext, attachmentAnchor: .point(.center), arrowEdge: .trailing) {\n                PopupView(height: $height) // &lt;- this \n                    .frame(width: UIDevice.isPad ? 400 : geometry.frame(in: CoordinateSpace.global).maxX - 16, height: height) // &lt;- this \n                    .presentationCompactAdaptation(.none)\n            }\n        }\n    }\n}\n\nstruct PopupView: View {    \n    @Binding var height: CGFloat // &lt;- this \n\n    var body: some View {\n        VStack(spacing: .zero) {\n            ...\n        }\n        .padding()\n        .overlay(\n            GeometryReader { geometry in\n                Color.clear\n                    .onAppear {\n                        height = ceil(geometry.size.height) // &lt;- this \n                    }\n            }\n        )\n    }\n}\n</code></pre>\n", "tags": ["ios", "swift", "swiftui", "popover", "swiftui-sheet"]}
{"id": "72a7c517-9a40-47a8-96cd-3ed447bfe49a", "text": "Q: multiprocessing.JoinableQueue.empty() seems broken A: <p>Try using <a href=\"https://github.com/python/cpython/issues/87302#issuecomment-3660276003\" rel=\"nofollow noreferrer\">the workaround from related python/cpython#87302</a>, replacing <code>multiprocessing.queues.Queue</code> with <code>multiprocessing.queues.JoinableQueue</code> as the parent. For me on Linux, this fixed your problem due to the redefined <code>empty()</code> via semaphore.</p>\n<p>As for the cause, I am not sure I can give a competent answer. <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue\" rel=\"nofollow noreferrer\"><code>multiprocessing.Queue</code></a> uses <a href=\"https://github.com/python/cpython/blob/0b8c348f2756c193d6bd2618cadbb90b2f218ccc/Lib/multiprocessing/connection.py#L1140-L1171\" rel=\"nofollow noreferrer\">pipe waiting via poll/select with zero timeout (on UNIX)</a> in <a href=\"https://github.com/python/cpython/blob/db098a475a47b16d25c88d95dbcf0c6572c68576/Lib/multiprocessing/queues.py#L126-L127\" rel=\"nofollow noreferrer\">its implementation of the <code>empty()</code> method</a>. It is possible that the pipe may not be ready to read due to buffering, some limits, or simply undocumented behavior (multiprocessing is truly hell).</p>\n<p>If you succeed in further localizing the issue, I suggest <a href=\"https://github.com/python/cpython/issues\" rel=\"nofollow noreferrer\">reporting it to the CPython team</a>.</p>\n<hr />\n<p><strong>Update.</strong> If we take a closer look, we can see that the <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.put\" rel=\"nofollow noreferrer\"><code>put()</code></a> method does not take effect immediately, but instead <a href=\"https://github.com/python/cpython/blob/db098a475a47b16d25c88d95dbcf0c6572c68576/Lib/multiprocessing/queues.py#L90-L94\" rel=\"nofollow noreferrer\">delegates pickling and subsequent insertion to the background thread</a>. Moreover, starting from a certain number of bytes, the thread hangs.</p>\n<pre class=\"lang-py prettyprint-override\"><code>#!/usr/bin/env python3\n\nimport sys\n\nfrom multiprocessing import Queue\n\nITEMS = 10_000  # or another large number\n\n\ndef main():\n    queue = Queue()\n\n    for i in range(ITEMS):\n        queue.put(i)\n\n    assert queue.qsize() == ITEMS\n\n\nif __name__ == &quot;__main__&quot;:\n    sys.exit(main())\n\n# program will hang until you press Control-C\n</code></pre>\n<p>Thus, the implementation makes the incorrect assumption that the pipe always contains at least some of the inserted items. This does not cause problems in simple cases due to the lack of context switching with a small number of processes, but has an unpleasant effect when there are enough of them.</p>\n<pre class=\"lang-py prettyprint-override\"><code>#!/usr/bin/env python3\n\nimport sys\n\nfrom multiprocessing import Process, Queue\n\nCONSUMERS = 5\nITEMS = CONSUMERS * 10_000  # or another large number\n\n\ndef consume(queue):\n    for _ in range(ITEMS // CONSUMERS):\n        if queue.empty():  # it should never be printed, but it will be\n            print(&quot;EMPTY!&quot;)\n\n        queue.get()\n\n\ndef main():\n    queue = Queue()\n\n    for i in range(ITEMS):\n        queue.put(i)\n\n    assert queue.qsize() == ITEMS\n\n    consumers = [\n        Process(target=consume, args=[queue], daemon=True)\n        for _ in range(CONSUMERS)\n    ]\n\n    for consumer in consumers:\n        consumer.start()\n\n    for consumer in consumers:\n        consumer.join()\n\n\nif __name__ == &quot;__main__&quot;:\n    sys.exit(main())\n</code></pre>\n<p>Essentially, the problem is that <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.qsize\" rel=\"nofollow noreferrer\"><code>qsize()</code></a> (and anything that emulates it) does not depend on the pipe's state, but <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.empty\" rel=\"nofollow noreferrer\"><code>empty()</code></a> checks its readiness. In situations where the pipe drains faster than the buffer is flushed, <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.empty\" rel=\"nofollow noreferrer\"><code>empty()</code></a> will return <code>True</code> earlier than expected.</p>\n<p>Okay, so how does the above apply to your example? Let us assume that in your example, one worker process, having received an item, always has time to insert all items into the underlying buffer in one timeslice (or something like that) immediately, and the next worker process wakes up as soon as the background thread of the previous process finishes inserting one item into the pipe. Then we get that regardless of how many items are left to serialize and write to the pipe, each process immediately empties the pipe and, due to the behavior of the <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.empty\" rel=\"nofollow noreferrer\"><code>empty()</code></a> method described above, sees that the queue is &quot;empty&quot; (when in fact the background threads simply have not had time to insert items from the buffers yet). This may be caused by some specific process scheduling policy.</p>\n<p>Is this a bug? Answers may vary. But this is definitely not expected behavior, and this point (the asymmetry of <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.qsize\" rel=\"nofollow noreferrer\"><code>qsize()</code></a>/<a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.full\" rel=\"nofollow noreferrer\"><code>full()</code></a> and <a href=\"https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.empty\" rel=\"nofollow noreferrer\"><code>empty()</code></a>; yes, your queue can be in two mutually exclusive states at the same time, which, by the way, is not true for queues from the <a href=\"https://docs.python.org/3/library/queue.html\" rel=\"nofollow noreferrer\"><code>queue</code></a> module) should at least be clarified in the documentation.</p>\n<hr />\n<p>Reported as <a href=\"https://github.com/python/cpython/issues/142837\" rel=\"nofollow noreferrer\">python/cpython#142837</a>.</p>\n", "tags": ["python", "multithreading", "queue"]}
{"id": "162b32eb-1c7d-4152-a214-f64df02541ad", "text": "Q: How to knit R markdown to word doc without code A: <p>If you only need the rendered content (text, tables, figures) in Word, you don\u2019t have to rely on R Markdown\u2019s Word output.</p>\n<p>An easier option is to export a clean Markdown version (no code blocks) and convert it directly to .docx using a Markdown \u2192 Word converter, e.g. <a href=\"https://www.markdown-to-word.online\" rel=\"nofollow noreferrer\">https://www.markdown-to-word.online</a>. This guarantees no code appears in the final document.</p>\n", "tags": ["r", "ms-word", "r-markdown", "output", "knitr"]}
{"id": "83783755-4dc2-4374-aba1-147d375155d3", "text": "Q: Is it possible to translate an input type=&quot;month&quot; with i18next? A: <p>Nah. The browser <code>&lt;input type=&quot;month&quot;&gt;</code> only likes their native language and ignores your fancy i18next setup. It doesnt care about your React translations. it will only show months in the user os language.</p>\n<p>Your options, good sir:</p>\n<ol>\n<li><p>Keep the native input -&gt; Users get a weird mix: your site in Portuguese, the popup in their OS languages.</p>\n</li>\n<li><p>Build your own picker -&gt; A lil hard to do, at least everythings in Portuguese!</p>\n</li>\n<li><p>Use a library -&gt; <code>react-datepicker</code> already maded this. Its like paying someone else to deal with your problem.</p>\n</li>\n</ol>\n", "tags": ["html", "internationalization", "tags", "i18next", "react-i18next"]}
{"id": "62fe2a02-2a6d-48f9-8bae-e29108e14893", "text": "Q: PowerShell Test-Connection output to file or color results A: <p>@santiago-squarzon I just wanted to follow-up with an update and give back to the community! Thanks to your fantastic and simple explanation and examples I was able to learn a lot and refine my script for mass device ping :) Thank you once again!</p>\n<p>Here is my final script:</p>\n<pre class=\"lang-php prettyprint-override\"><code>## color definition\n$map = @{\n    $false = &quot;$([char] 27)[91mOFFLINE$([char] 27)[0m&quot;\n    $true  = &quot;$([char] 27)[92mONLINE$([char] 27)[0m&quot;\n}\n\n## input file\n$devices = Get-Content -Path C:\\temp\\devices.txt\n\n## fast parallel ping\n$pingtask = @{}\n$timeout = 1000\n\nforeach ($device in $devices) {\n    $ping = New-Object System.Net.NetworkInformation.Ping\n    $pingtask[$device] = $ping.SendPingAsync($device, $timeout)\n}\n\n## wait for all pings to complete\n[System.Threading.Tasks.Task]::WaitAll($pingtask.Values)\n\n## collect ping results\n$pingstatus = @{}\nforeach ($device in $pingtask.Keys) {\n    try {$pingstatus[$device] = ($pingtask[$device].Result.Status -eq 'Success')}\n    catch {$pingstatus[$device] = $false}\n}\n\n## results\nforeach ($device in $devices) {\n    [pscustomobject]@{\n        DeviceName = $device\n        Status     = $map[$pingstatus[$device]]\n    }\n}\n</code></pre>\n<p>Result:</p>\n<p><a href=\"https://i.sstatic.net/pzzkRTCf.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/pzzkRTCf.png\" alt=\"color results\" /></a></p>\n<p>And here is the version that does the same except instead of displaying the results in PowerShell it saves them to CSV (without the colors):</p>\n<pre class=\"lang-php prettyprint-override\"><code>## input file\n$devices = Get-Content -Path C:\\temp\\devices.txt\n\n## fast parallel ping\n$pingtask = @{}\n$timeout = 1000\n\nforeach ($device in $devices) {\n    $ping = New-Object System.Net.NetworkInformation.Ping\n    $pingtask[$device] = $ping.SendPingAsync($device, $timeout)\n}\n\n## wait for all pings to complete\n[System.Threading.Tasks.Task]::WaitAll($pingtask.Values)\n\n## collect ping results\n$pingstatus = @{}\nforeach ($device in $pingtask.Keys) {\n    try {$pingstatus[$device] = ($pingtask[$device].Result.Status -eq 'Success')}\n    catch {$pingstatus[$device] = $false}\n}\n\n## results\n$result = foreach ($device in $devices) {\n    [pscustomobject]@{\n        DeviceName = $device\n        Status     = if ($pingstatus[$device]) {'ONLINE'} else {'OFFLINE'}\n    }\n}\n\n## export to csv\n$result | Export-Csv C:\\temp\\pinged_devices.csv -NoTypeInformation\n</code></pre>\n<p>Result:</p>\n<p><a href=\"https://i.sstatic.net/BxZqeTzu.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/BxZqeTzu.png\" alt=\"CSVresult\" /></a></p>\n<p>Parsed into excel:</p>\n<p><a href=\"https://i.sstatic.net/Yn13z9x7.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Yn13z9x7.png\" alt=\"parsedexcel\" /></a></p>\n", "tags": ["powershell", "output"]}
{"id": "5daf2759-c158-4be5-9f86-fd32e46f63b2", "text": "Q: Branch predictor training depends on call site? (Spectre experiment) A: <p>This isn't a discussion/opinion question; it has an objective answer. Please delete this and re-ask it as a proper question so I can post an answer where it belongs, since that's currently the only way to fix the problem.</p>\n<p>(Stack overflow is running a badly-designed experiment which misleads people into asking questions like this as opinion-based, not real questions. <em><a href=\"https://meta.stackoverflow.com/questions/435293/opinion-based-questions-alpha-experiment-on-stack-overflow\">Opinion-based questions alpha experiment on Stack Overflow</a></em> Many of the people in charge at SO seem to think that debugging questions were the only kind of normal questions previous allowed, or something like that. Many of them are completely out of touch with the community that actually uses Stack Overflow. Also, they started this experiment without the ability to flip a question from this bad format to normal Q&amp;A.)</p>\n<p>Ryzen 7 5700X3D is a Zen 3 chip. <a href=\"https://en.wikichip.org/wiki/amd/microarchitectures/zen_3#Key_changes_from_Zen_2\" rel=\"nofollow noreferrer\">https://en.wikichip.org/wiki/amd/microarchitectures/zen_3#Key_changes_from_Zen_2</a> - BPU mostly unchanged from Zen 2.  <a href=\"https://en.wikichip.org/wiki/amd/microarchitectures/zen_2#Branch_Prediction_Unit\" rel=\"nofollow noreferrer\">https://en.wikichip.org/wiki/amd/microarchitectures/zen_2#Branch_Prediction_Unit</a></p>\n<p>In both your cases, <code>__attribute__((noinline))</code> should be working.  The JCC has the same address regardless of the call-site it's reached from, assuming it's actually not inlined.  The <code>call</code> instructions should be unconditional direct so not part of the taken / not-taken history the IT-TAGE predictor uses to index a prediction for the JCC.</p>\n", "tags": ["x86-64", "cpu-architecture", "amd-processor", "branch-prediction", "spectre"]}
{"id": "96fe4134-6699-42bb-aef8-305a6572c4d3", "text": "Q: What&#39;s the Lua equivalent of Python&#39;s endswith()? A: <p>I have created a new endswith function that mimics python behavior by using sub string indexing with the provided length:</p>\n<pre><code>local function endswith(haystack, needle)\n    local suffix = string.sub(haystack, -(#needle))\n    return suffix == needle\nend\n</code></pre>\n<p>This way, you can test something like:</p>\n<pre><code>local result = endswith('file.exe', '.exe')\n</code></pre>\n<p>It works with any string length</p>\n", "tags": ["lua", "nmap"]}
{"id": "913d1976-2b6f-4206-9287-3b50bc0636fa", "text": "Q: How to poll another server from Node.js? A: <p>You're absolutely right that setInterval() won't work here. It dies with restarts, deploys, or crashes, leaving users hanging. This is a classic long-running workflow problem. You need something that survives server restarts and can pick up where it left off.</p>\n<h3>The core pattern</h3>\n<p>Whatever tool you use, the approach is the same:</p>\n<p>1. Persist the state - store the operation ID, current step, user info in your database<br />\n2. Use background jobs - queue a &quot;check status&quot; task that can reschedule itself<br />\n3. Make it resumable - if the server crashes mid-process, the next worker can pick it up</p>\n<h3>BullMQ implementation</h3>\n<p>Here's how this looks with BullMQ:</p>\n<pre><code>javascript\n// Start the workflow\nconst job = await importQueue.add('shopify-bulk-import', {\n  userId: user.id,\n  shopifyOperationId: operationId,\n  step: 'POLLING'\n});\n// Worker handles each step\nimportQueue.process('shopify-bulk-import', async (job) =&gt; {\n  const { userId, shopifyOperationId, step } = job.data;\n  if (step === 'POLLING') {\n    const status = await shopify.getBulkOperationStatus(shopifyOperationId);\n    if (status.state === 'COMPLETED') {  \n      // Move to next step  \n      return await importQueue.add('shopify-bulk-import', {  \n        userId,  \n        downloadUrl: status.url,  \n        step: 'DOWNLOADING'  \n      }, { delay: 1000 });  \n    }  \n  \n    // Still running, check again in 30 seconds  \n    return await importQueue.add('shopify-bulk-import', job.data, { delay: 30000 });  \n  }\n  if (step === 'DOWNLOADING') {\n    const data = await downloadAndProcess(job.data.downloadUrl);\n    await saveToDatabase(userId, data);\n    await sendCompletionEmail(userId);\n  }\n});\n</code></pre>\n<p>This works well and handles restarts properly.</p>\n<h3>Where it gets complex</h3>\n<p>Once you have multiple workflows like this, you end up rebuilding the same patterns:<br />\n\u2022 Step-by-step state management<br />\n\u2022 Retry logic with backoff<br />\n\u2022 &quot;What step is this stuck on?&quot; visibility<br />\n\u2022 Replaying from a specific step when something breaks</p>\n<p>BullMQ handles job execution, but you're still managing workflow state manually.</p>\n<h3>Alternative approach</h3>\n<p>Some teams use workflow platforms for this stuff - things like Temporal, AWS Step Functions, or automation platforms like SiloWorker. You define the steps<br />\ndeclaratively:</p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;steps&quot;: [\n        {\n            &quot;type&quot;: &quot;http&quot;,\n            &quot;config&quot;: {\n                &quot;url&quot;: &quot;https://{{shop}}.myshopify.com/admin/api/2023-01/graphql.json&quot;,\n                &quot;method&quot;: &quot;POST&quot;,\n                &quot;body&quot;: &quot;mutation { bulkOperationRunQuery(...) }&quot;\n            }\n        },\n        {\n            &quot;type&quot;: &quot;conditional&quot;,\n            &quot;condition&quot;: &quot;{{response.data.bulkOperationRunQuery.bulkOperation.status}} === 'RUNNING'&quot;,\n            &quot;then&quot;: [\n                { &quot;type&quot;: &quot;delay&quot;, &quot;seconds&quot;: 30 },\n                { &quot;type&quot;: &quot;goto&quot;, &quot;step&quot;: 0 }\n            ],\n            &quot;else&quot;: [\n                { &quot;type&quot;: &quot;http&quot;, &quot;url&quot;: &quot;{{response.data.bulkOperationRunQuery.bulkOperation.url}}&quot; },\n                { &quot;type&quot;: &quot;database&quot;, &quot;query&quot;: &quot;INSERT INTO orders ...&quot; },\n                { &quot;type&quot;: &quot;sendgrid&quot;, &quot;template&quot;: &quot;import-complete&quot; }\n            ]\n        }\n    ]\n}\n</code></pre>\n<p>The platform handles state persistence, retries, and step orchestration automatically.</p>\n<p>Tradeoff: You give up some control but avoid a lot of operational complexity. For a Shopify app handling hundreds of imports, that's often worth it.</p>\n<h3>Bottom line</h3>\n<p>BullMQ is solid and will work. Just be ready to build workflow management on top of it as your needs grow. The polling pattern you're describing is exactly what these<br />\ntools are designed for.</p>\n", "tags": ["node.js", "express", "shopify"]}
