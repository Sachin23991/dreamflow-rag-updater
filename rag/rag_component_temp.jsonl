{"id": "db4910d3-9c8e-46cc-92a4-1bb8eaf7fc24", "text": "Q: git repo structure for a project with multiple variants? A: <p>My experience working with source control and git SCM specifically should be reasonably applicable to your question.  Though I am still somewhat novice at git practices, it has been found to be a very useful and widely used software.</p>\n<p>One way to manage your main version along with multiple secondary versions of a product situation is to use the very established git &quot;branch&quot; functionality.  It allows the programmer to easily switch a development environment file tree between named different versions of a code repository and then &quot;merge&quot; the differences back into the main version if they are tested and accepted for that version.  Specific related questions you might ignore are things like branches of branches therein allowing version-specific simultaneous feature development and testing.  Multiple file directory copies of a repository with different branches can be simpler from the source control perspective as discussions around extended versioning practices are easily confused.</p>\n<p>Good software versioning practices are worth emphasizing as they could be very useful for your situation.  By identifying things like feature development and customer specific development with good version numbers or names and versioning practices, many aspects of software development are understandably easier.  Further, combining the use of git branch and software versioning practices might be useful in your situation.  For example the main version of the software would have a major version number where customer and special constraint specific versions might have minor numbers or identifying branch names, thereby implying that a specific specially constrained branch should be future development compatible by traced derivation to a certain major version number but probably not later major versions.</p>\n<p>If you are already using the git branch and merge workflow in a way that is problematic or incompatible to your sensitive version isolation requirements, you might manage that constraining requirement by establishing and enforcing repository management guidelines, or you might consider alternative source control management software that has better liability guarantees than open source software.  No recommendations in this context would be appropriate.</p>\n", "tags": ["git"]}
{"id": "c00d3273-56c8-4f8a-9258-6b37543bebae", "text": "Q: CGI in Perl: how to access the original HTTP headers A: <p>CGI scripts don't receive HTTP requests; the receive CGI requests. But that in includes most of not all HTTP headers (depending on your server).</p>\n<p>The request method is available as <code>$ENV{ REQUEST_METHOD }</code>.</p>\n<p>The request uri is available as <code>$ENV{ REQUEST_URI }</code>.</p>\n<p>Like you said, the Content-Type header is available as <code>$ENV{ CONTENT_TYPE }</code>.</p>\n<p>Other headers Xxx-Yyy are (optionally) available as <code>$ENV{ HTTP_XXX_YYY }</code>.</p>\n<p>Given the above, we can recreate the HTTP headers as follows:</p>\n<pre class=\"lang-perl prettyprint-override\"><code>sub cgi_to_http { $_[0] =~ s/^HTTP_//r =~ tr/_/-/r =~ s/\\b\\w+/\\L\\u$&amp;/gr }\n\nmy %headers =\n   map { cgi_to_http( $_ ) =&gt; $ENV{ $_ } }\n      grep { $_ eq &quot;CONTENT_TYPE&quot; or /^HTTP_/ }\n         keys( %ENV );\n</code></pre>\n<p>The <code>s/\\b\\w+/\\L\\u$&amp;/g</code> part converts the capitalization of the key to its canon form. This is optional since HTTP headers are case-insensitive.</p>\n", "tags": ["perl", "http", "header", "cgi"]}
{"id": "328adca1-44b6-4383-a89a-420eb2d1b90f", "text": "Q: How do I put ggplot annotations onto my figure legend? A: <p>Try to use <code>geom_segment()</code>.</p>\n<pre><code>data_all &lt;- read.table(header = TRUE, text = &quot;\n    Concentration   Distance    Genotype\n    0   1   Q24\n    0   4   Q24\n    5   1   Q24\n    5   0   Q24\n    10  0   Q24\n    10  1   Q24\n    0   1   Q35\n    0   3   Q35\n    0   4   Q35\n    5   0   Q35\n    5   1   Q35\n    10  0   Q35\n    10  2   Q35\n                  &quot;)\n\nseg &lt;- data.frame(x = c(0.8, 1.8, 0.8, 1.8), y = c(2.86, 1.86, 2.41, 2.16),\n                  xend = c(1.2, 2.2, 1.2, 2.2), yend = c(2.86, 1.86, 2.41, 2.16),\n                  color = c(&quot;Q24&quot;, &quot;Q24&quot;, &quot;Q35&quot;, &quot;Q35&quot;))\n\nlibrary(ggplot2)\nggplot(data_all,aes(x=Concentration, y=Distance, colour=Genotype))+\n    geom_jitter(width=0.3, height=0, show.legend=TRUE)+\n    theme(panel.background = element_rect(fill = &quot;white&quot;)) +\n    theme(axis.line.x = element_line(color = &quot;black&quot;),\n          axis.line.y = element_line(color = &quot;black&quot;))+\n    scale_y_continuous(breaks=seq(0, 8, 1))+\n    xlab(&quot;Caffeine concentration (mM)&quot;)+\n    ylab(&quot;Distance travelled (arbitrary units)&quot;)+\n    geom_segment(data = seg, aes(x = x, y = y, xend = xend, yend = yend, color = color)) +\n    scale_colour_manual(values = c(&quot;Q24&quot; = &quot;black&quot;, &quot;Q35&quot; = &quot;green3&quot;))\n</code></pre>\n", "tags": ["r", "ggplot2", "legend", "mean"]}
{"id": "9faae5c4-199c-4f00-9530-7c7fde5511b8", "text": "Q: pydantic initialize numpy ndarray A: <p>As jkinkead says, you should use BaseModel rather than dataclass, but if you still want to utilize pydantic validation.</p>\n<p>Maybe you can try my open source lib: sympydantic, It's developing period yet, only support python&gt;=3.12, but I believe `sympydantic` can provide you convinence. \ud83d\ude09, you can install it from github.</p>\n<blockquote>\n<p>pip install <a href=\"https://github.com/HirasawaGen/sympydantic.git\" rel=\"nofollow noreferrer\">https://github.com/HirasawaGen/sympydantic.git</a></p>\n</blockquote>\n<p>Then you can use it:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nfrom sympydantic import NDArray\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass TestNumpyArray:\n    numpyArray: NDArray[np.float32] = np.zeros(10)\n\n</code></pre>\n<p>If you init this class as:</p>\n<blockquote>\n<p>TestNumpyArray(numpyArray=[0, 1, 2, 3, 4, 5])</p>\n</blockquote>\n<p>The field <code>numpyArray</code> will be automatically cast from list to ndarray, the dtype will cast to float32 as well.</p>\n<p>You can even validate the shape.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from typing import Annotated\nimport numpy as np\nfrom sympydantic import NDArray, tensorshape\nfrom pydantic import BaseModel\n\n\nclass TestNumpyArray(BaseModel):\n    numpyArray: Annotated[NDArray, tensorshape[5:20]] = np.zeros(10)\n\n</code></pre>\n<p>then If you want to initialize numpyArray as a numpy array whose length is less than 5, or greater equal than 20, it will be denied by pydantic.</p>\n<p>pydantic will raise an error like:</p>\n<pre class=\"lang-none prettyprint-override\"><code>pydantic_core._pydantic_core.ValidationError: 1 validation error for TestNumpyArray\nnumpyArray\n  The 0-th dimension ofthis tensor-like object should be greater than or equal to 5. You provide 2. [type=greater_than_equal, input_value=array([0., 0.]), input_type=ndarray]\n</code></pre>\n", "tags": ["python-3.x", "numpy-ndarray", "pydantic"]}
{"id": "cc966023-4b74-4b38-ba98-2db9cd4763dc", "text": "Q: Is it possible to change subnet in Azure AKS deployment? A: <p>In my case I have AKS clusters (using vmss pools) that are set by Terraform via rancher.</p>\n<p>I created a vnet and subnet as Terraform resources and managed to change my initial network ranges.<br />\nThis should also be possible using just the Azure portal.</p>\n<p><code>Note: Don`t remove vnet or subnet resources itself, just change CIDR settings!</code></p>\n<p>Steps for VMSS:</p>\n<ol>\n<li><p>Add your desired <code>cidr</code> to <code>existing</code> vnet and subnet.</p>\n</li>\n<li><p>Stop cluster - all nodes in vmss pools must be down to remove existing subnet and network cidrs.</p>\n</li>\n<li><p>Remove old vnet/subnet <code>cidrs</code> via portal or Terraform - keep just the new one.</p>\n</li>\n<li><p>Start cluster - all nodes/pods should use now the new network ranges.</p>\n</li>\n</ol>\n<p>Limitation:</p>\n<ol>\n<li><p>It's possible to change node/pods IPs but not Service CIDR or dns service IP.<br />\n(terraform will accept the new values and execute plan but they are reverted/ignored).<br />\nService CIDR use overlay network and as such only is visible from cluster side, so not big issue.<br />\nUseful information: <a href=\"https://www.youtube.com/watch?v=hoNBlGVD1CY\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=hoNBlGVD1CY</a></p>\n</li>\n<li><p>After making the changes, I noticed that all my pods see IP addresses from my new network range from the nginx controller instead of IP addresses from outside visitors<br />\n(even when I have <code>externalTrafficPolicy: Local</code> for my load balancing service).<br />\nAccording <a href=\"https://spidermilk.co/posts/aks-masq/\" rel=\"nofollow noreferrer\">https://spidermilk.co/posts/aks-masq/</a> I added <code>azure-ip-masq-agent-config</code> with my new cluster subnet, this update azure-ip-masq-agent configuration and solve ingress issues.</p>\n</li>\n</ol>\n<pre class=\"lang-none prettyprint-override\"><code>Note: Don`t remove vnet resource but this time remove subnets (details below)\n</code></pre>\n<p>Steps for AvailabilltySet:</p>\n<ol>\n<li><p>Add your desired <code>cidr</code> to <code>existing</code> vnet.</p>\n</li>\n<li><p>For AvailabilltySet pool it's not possible to stop cluster but instead you can find corresponding virtual machine and stop it.<br />\nFor minimal work scale cluster down if possible to even one node.</p>\n</li>\n<li><p>Add temporary <code>subnet</code> with new <code>cidr</code></p>\n</li>\n<li><p>Find <code>nics</code> of your node and reconfigure it to use new subnet.</p>\n</li>\n<li><p>Repeat step 4 for each nic/node that you have.<br />\nWhen all your <code>nics</code> be reconfigured you be able to do next step.</p>\n</li>\n<li><p>Recreate old subnet under the same name but with new final desire <code>cidr</code><br />\n(keep the same settings like <code>nsg</code> or other things that you have set for old subnet)</p>\n</li>\n<li><p>Again reconfigure all <code>nodes</code> and his <code>nics</code> to use final subnet.</p>\n</li>\n<li><p>Remove temporary subnet, and remove old <code>cidr</code> from <code>vnet</code></p>\n</li>\n<li><p>Start nodes.</p>\n</li>\n<li><p>Edit each node using <code>kubectl</code> and change the annotation: <code>alpha.kubernetes.io/provided-node-ip:</code> to the primary IP address assigned to the network card.</p>\n<p>If you don't do this, some of your pods will still report the old IP addresses, and you won't be able to use debugging commands such as: <code>kubectl debug node/your_node_name -it --image=mcr.microsoft.com/cbl-mariner/busybox:2.0</code> because the proxy will try to connect via the old IP.</p>\n</li>\n<li><p>Similar to VMSS scenario, check if you need edit/add <code>azure-ip-masq-agent-config</code></p>\n</li>\n</ol>\n", "tags": ["azure", "kubernetes", "azure-aks"]}
