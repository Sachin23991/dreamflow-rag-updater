{"id": "95552582-5538-4ff3-bc60-20b44fbfe56b", "text": "Q: Can&#39;t directly update a dict deepcopy A: <p>Why? Because <a href=\"https://docs.python.org/3/library/stdtypes.html#dict.update\" rel=\"nofollow noreferrer\"><code>dict.update</code></a> always operates in place and returns <code>None</code>.</p>\n<p>How? You can invoke an <a href=\"https://docs.python.org/3/reference/datamodel.html#object.__ior__\" rel=\"nofollow noreferrer\">in-place union</a>:</p>\n<pre><code>&gt;&gt;&gt; d0 = {&quot;a&quot;: [1]}\n&gt;&gt;&gt; d1 = deepcopy(d0).__ior__([(&quot;b&quot;, 2)])\n&gt;&gt;&gt; d0[&quot;a&quot;].append(3)\n&gt;&gt;&gt; print(d0, d1)\n{'a': [1, 3]} {'a': [1], 'b': 2}\n</code></pre>\n<p>Note that in example cases shown in the original question, a deepcopy isn't required. The values are immutable, so you can safely <em>shallow</em> copy and update in a single operation, which will be simpler and more efficient:</p>\n<pre><code>&gt;&gt;&gt; d0 = {&quot;a&quot;: 1}\n&gt;&gt;&gt; d1 = d0 | {&quot;b&quot;: 2}  # or: d1 = {**d0, &quot;b&quot;: 2}\n&gt;&gt;&gt; print(d0, d1)\n{'a': 1} {'a': 1, 'b': 2}\n</code></pre>\n", "tags": ["python", "dictionary", "copy"]}
{"id": "200a97bb-3fcb-4ce7-8d61-c4589fbfdd22", "text": "Q: Using CodeIgniter&#39;s base_url() in a hyperlink href value leads to 404 error A: <p><strong>Set base url in config.php</strong></p>\n\n<pre><code>$config['base_url'] = (isset($_SERVER['HTTPS']) ? \"https://\" : \"http://\") . $_SERVER['HTTP_HOST'] . preg_replace('@/+$@', '', dirname($_SERVER['SCRIPT_NAME'])) . '/';\n</code></pre>\n", "tags": ["php", "codeigniter", "hyperlink", "url-routing"]}
{"id": "c8cf60ab-419b-4e10-ba1e-8d56f5bc0a66", "text": "Q: How is UTF-8 safe relative to ASCII chars A: <p>Before unicode, lots of software treated strings as sequences of &quot;extended ascii&quot; bytes, some software still does so to this day. Bytes in the range 0-127 were defined as ascii characters and may or may not have special meanings. Bytes in the range 128-255 did not have any special meaning to the software, they were simply passed through unchanged.</p>\n<p>Obviously software or hardware that actually rendered text to an image would need to care about what the codes actually mean, as would software or hardware reading keystrokes from the keyboard and turning them into character codes but the bulk of the system didn't need to care.</p>\n<p>In particular, Unix systems usually used serial terminals, so on a unix system the process of assigning human-meaning to extended character codes was usually a matter for the terminal. Not for the computer.</p>\n<p>This was far from a perfect system, if the writer and the reader used different character sets, then &quot;mojibake&quot; was likely to result. Some systems tried to solve this problem by adding a metadata field to indicate the encoding that was in-use. For example MIME introduced the &quot;charset&quot; field to indicate the encoding used in emails (and later HTTP).</p>\n<p>Some languages did not fit in the space available in an &quot;extended ASCII&quot; encoding. Most notably Chinese, Japanese and Korean. These languages resorted to &quot;Multibyte&quot; encodings, in which some characters required more than one byte to store.</p>\n<p>In some of these encodings byte values in the range 0-127 could appear as part of a multi-byte character. This could cause problems, a programming language may incorrectly interpret a byte as a piece of programming syntax, a file system may incorrectly interpret a byte as a separator in a directory path, a structured text format may incorrectly interpret a byte as a field seperator.</p>\n<p>In general, there were two approaches taken to unicode adoption.</p>\n<p>Microsoft adopted Unicode through new formats and APIs. For file names, they did this as part of the same process as introducing &quot;long filename&quot; support. 8-bit APIs were retained but increasingly came to be viewed as legacy.</p>\n<p>The Unix world on the other hand already had support for long filenames. It also lacked a single large controlling entity that could push through such a radical change. So there was a desire to be able to have a Unicode encoding that could be treated as &quot;just another extended ascii variant&quot;.</p>\n<p>The ISO had proposed 8-bit encoding for Unicode called UTF-1, but it did not protect most printable Ascii characters. It also had performance issues in encoding and decoding. A proposal was circulated, some modifications were suggested and UTF-8 as we know it was born.</p>\n<p>There were pros and cons to both approaches. The MS approach let existing software and users keep working as before, whereas the Unix approach would require users to switch their environment to UTF-8 and manually convert their existing data. On the other hand the Unix approach meant that existing software could be used with Unicode text immediately (possibly with a few glitches) while the Windows appoach lead to much software remaining stuck with legacy encodings well into the 2000s.</p>\n", "tags": ["unicode", "utf-8", "character-encoding", "ascii"]}
{"id": "ca139a25-2394-4fde-9f51-c3ccb40fdbf5", "text": "Q: Is there a Google Sheets formula to put the name of the sheet into a cell? A: <p>Not sure whether this option has been introduced only recently, but there is a <code>sheet</code> parameter that can be passed on to the <code>CELL()</code> function. It returns the current sheet name!</p>\n<p>It's not even <a href=\"https://support.google.com/docs/answer/3267071\" rel=\"nofollow noreferrer\">documented</a>.</p>\n<p><a href=\"https://i.sstatic.net/Hl0CnjLO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Hl0CnjLO.png\" alt=\"enter image description here\" /></a></p>\n", "tags": ["google-sheets"]}
