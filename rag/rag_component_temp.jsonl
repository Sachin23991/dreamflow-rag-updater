{"id": "41f9208d-0c06-4364-a12d-b84de26f6693", "text": "Q: How to play ULAW 8kHz sound in Java today? A: <p>My problem, it turned out, was with the ALSA-configuration. The <code>javax.sound.sampled.AudioSystem.getMixerInfo()</code> was returning an <em>empty list</em>, so <em>no format</em> would play.</p>\n<p>Installing the <a href=\"https://www.freshports.org/audio/alsa-plugins\" rel=\"nofollow noreferrer\">audio/alsa-plugins</a> port solved it all. The sampling rate, which Google's AI made me think was the problem, was not...</p>\n<p>For example, using Python's <code>audiotest.au</code> as well as Oracle's JavaSoundDemo, my code reports:</p>\n<pre class=\"lang-none prettyprint-override\"><code>/opt/lib/python2.7/email/test/data/audiotest.au: input format: ULAW 8012.0 Hz, 8 bit, mono, 1 bytes/frame\n/opt/lib/python2.7/email/test/data/audiotest.au: using format: PCM_SIGNED 8012.0 Hz, 16 bit, mono, 2 bytes/frame, little-endian\nJavaSoundDemo/audio/1-welcome.wav: input format: PCM_SIGNED 11025.0 Hz, 16 bit, mono, 2 bytes/frame, little-endian\nJavaSoundDemo/audio/1-welcome.wav: using format: PCM_SIGNED 11025.0 Hz, 16 bit, mono, 2 bytes/frame, little-endian\nJavaSoundDemo/audio/spacemusic.au: input format: ULAW 8000.0 Hz, 8 bit, mono, 1 bytes/frame\nJavaSoundDemo/audio/spacemusic.au: using format: PCM_SIGNED 8000.0 Hz, 16 bit, mono, 2 bytes/frame, little-endian\n</code></pre>\n<p>and the sound plays for all of them.</p>\n", "tags": ["java", "javasound"]}
{"id": "fbfea69d-4243-44d3-b304-d6143d43a99e", "text": "Q: Contact Form 7 occasionally receives broken uploaded images (1 byte files) \u2013 how to debug? A: <p>CF7 is a popular plugin for wordpress which allows user to input data, including file uploading.</p>\n<p>However, for some versions (e.g. contact-form-7 5.1.6), it has vulnerability so that a malicious user can upload data by curl.</p>\n<p><strong>Practically there should not be a JPEG file (an image) with 1-byte size, so I believe a user is trying to hack (or test hacking) your site.</strong></p>\n<p>See <a href=\"https://www.exploit-db.com/exploits/48062\" rel=\"nofollow noreferrer\">here_for_details</a></p>\n<p>The exploit is something like:</p>\n<pre><code>&lt;?php\n\n$shahab=&quot;file.jpg&quot;;\n$ch = curl_init(&quot;https://[your-domain-name]/wp-content/plugins/contact-form-7/modules/file.php&quot;);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS,\narray('zip'=&gt;&quot;@$shahab&quot;));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n$result = curl_exec($ch);\ncurl_close($ch);\nprint &quot;$result&quot;;\n?&gt;\n</code></pre>\n<p>I suggest you carefully check whether the version of CF7 you are using is still having this vulnerability. If so, then try to update to the latest version which already fixed it (if available), or replace it with another plugin which does not have this issue.</p>\n", "tags": ["php", "wordpress", "file", "upload"]}
{"id": "340bdeef-c853-4e4a-99f4-551220895d3d", "text": "Q: How to query columns that are lists or dicts? A: <p>Querying Dictionaries:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndata = {'col_dict': [{'id': 1, 'val': 'A'}, {'id': 2, 'val': 'B'}, {'id': 3, 'val': 'A'}]}\ndf = pd.DataFrame(data)\n\n# Query rows where the 'val' inside the dictionary is 'A'\nfiltered_rows = df[df['col_dict'].str['val'] == 'A']\nprint(&quot;Querying Dictionaries (using .str['key']):&quot;)\nprint(filtered_rows)\nprint(&quot;\\n&quot; + &quot;=&quot;*50 + &quot;\\n&quot;)\n</code></pre>\n<p>Querying Lists:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndata = {'col_list': [[1, 2, 3], [4, 5], [6, 7, 8]]}\ndf = pd.DataFrame(data)\n\n# Check if the list contains a specific value (e.g., 5)\ndf['contains_5'] = df['col_list'].apply(lambda x: 5 in x)\nfiltered_rows = df[df['contains_5']]\nprint(&quot;Querying Lists (using .apply(lambda x: value in x)):&quot;)\nprint(filtered_rows)\nprint(&quot;\\n&quot; + &quot;=&quot;*50 + &quot;\\n&quot;)\n</code></pre>\n<hr />\n<p>So adding this to the JSON data, it would look like:</p>\n<pre><code>import pandas as pd\nimport json\n\njson_data = &quot;&quot;&quot;\n[\n    {\n      &quot;id&quot;: 1,\n      &quot;name&quot;: &quot;John Doe&quot;,\n      &quot;age&quot;: 30,\n      &quot;email&quot;: &quot;john.doe@example.com&quot;,\n      &quot;isStudent&quot;: false,\n      &quot;hobbies&quot;: [&quot;reading&quot;, &quot;gaming&quot;, &quot;hiking&quot;],\n      &quot;address&quot;: {\n        &quot;street&quot;: &quot;123 Main St&quot;,\n        &quot;city&quot;: &quot;Anytown&quot;,\n        &quot;country&quot;: &quot;USA&quot;\n      }\n    },\n    {\n      &quot;id&quot;: 2,\n      &quot;name&quot;: &quot;Jane Smith&quot;,\n      &quot;age&quot;: 25,\n      &quot;email&quot;: &quot;jane.smith@example.com&quot;,\n      &quot;isStudent&quot;: true,\n      &quot;hobbies&quot;: [&quot;painting&quot;, &quot;yoga&quot;, &quot;photography&quot;],\n      &quot;address&quot;: {\n        &quot;street&quot;: &quot;456 Oak Ave&quot;,\n        &quot;city&quot;: &quot;Somewhere&quot;,\n        &quot;country&quot;: &quot;Canada&quot;\n      }\n    },\n    {\n      &quot;id&quot;: 3,\n      &quot;name&quot;: &quot;Bob Johnson&quot;,\n      &quot;age&quot;: 42,\n      &quot;email&quot;: &quot;bob.johnson@example.com&quot;,\n      &quot;isStudent&quot;: false,\n      &quot;hobbies&quot;: [&quot;cooking&quot;, &quot;fishing&quot;, &quot;gardening&quot;],\n      &quot;address&quot;: {\n        &quot;street&quot;: &quot;789 Pine Rd&quot;,\n        &quot;city&quot;: &quot;Otherville&quot;,\n        &quot;country&quot;: &quot;UK&quot;\n      }\n    },\n    {\n      &quot;id&quot;: 4,\n      &quot;name&quot;: &quot;Alice Chen&quot;,\n      &quot;age&quot;: 28,\n      &quot;email&quot;: &quot;alice.chen@example.com&quot;,\n      &quot;isStudent&quot;: false,\n      &quot;hobbies&quot;: [&quot;coding&quot;, &quot;chess&quot;, &quot;traveling&quot;],\n      &quot;address&quot;: {\n        &quot;street&quot;: &quot;321 Maple Blvd&quot;,\n        &quot;city&quot;: &quot;Techcity&quot;,\n        &quot;country&quot;: &quot;USA&quot;\n      }\n    },\n    {\n      &quot;id&quot;: 5,\n      &quot;name&quot;: &quot;David Wilson&quot;,\n      &quot;age&quot;: 19,\n      &quot;email&quot;: &quot;david.wilson@example.com&quot;,\n      &quot;isStudent&quot;: true,\n      &quot;hobbies&quot;: [&quot;basketball&quot;, &quot;music&quot;, &quot;movies&quot;],\n      &quot;address&quot;: {\n        &quot;street&quot;: &quot;654 Cedar Ln&quot;,\n        &quot;city&quot;: &quot;University Town&quot;,\n        &quot;country&quot;: &quot;Australia&quot;\n      }\n    }\n]\n&quot;&quot;&quot;\n\ndf = pd.read_json(json_data)\n\n# Query for 'gaming' in 'hobbies' using .apply()\ngaming_hobby_condition = df['hobbies'].apply(lambda x: 'gaming' in x)\n\n# Query for 'Anytown' in 'address.city' using .apply() to access dict key\n# Note: .str[] doesn't work directly here because 'address' itself is a dict, not a Series of strings.\n# We need to access the 'city' key within each dictionary.\ncity_condition = df['address'].apply(lambda x: x['city'] == 'Anytown')\n\n# Combine the conditions\nresult = df[gaming_hobby_condition &amp; city_condition]\n\nprint(&quot;Result using your suggested Pandas methods on the original data:&quot;)\nprint(result)\n</code></pre>\n<p><strong>Note</strong>: The <code>.str</code> accessor in Pandas is primarily designed for string operations and isn't guaranteed to work reliably on non-string data types, such as direct dictionary or list objects.</p>\n", "tags": ["python", "pandas"]}
{"id": "5a5bbfdb-f61d-40e9-8fd9-ab65f247857b", "text": "Q: Why using dirname in find command gives dots for each match? A: <p>You do not have to call <code>dirname()</code> for each file found.</p>\n<p>With GNU find, it is faster to use <code>-printf</code>:</p>\n<pre><code>find /path -type f -iname &quot;*.ext&quot; -printf &quot;%h\\n&quot;\n</code></pre>\n", "tags": ["linux", "bash", "unix", "find"]}
{"id": "6295bef7-9b76-4709-9912-ab537aba0967", "text": "Q: trying to read bigquery array colum and passing it as columns to fetch from spark dataframe A: <p>I dont have scala in my machine with help of databricks community i replicated and execute this code</p>\n<p>follow the same in scala</p>\n<p>Wrong approach fails because <code>concat_ws(',', column_list)</code> in your code snippet returns a single string &quot;BH_TCHH,BH_TCHF&quot;, which is not a valid column name in dfRaw.<br />\n<em><strong>Correction:</strong></em><br />\nExtract the list of column names from bqDf using <code>.collect()[0][0].</code><br />\nUse <code>[&quot;ts&quot;, &quot;id&quot;] + correct_column_list in dfRaw.select().</code><br />\nThis selects the actual columns, not a single string.</p>\n<pre class=\"lang-py prettyprint-override\"><code>#   replicate your BigQuery ARRAY&lt;STRING&gt; output\nbqDf = spark.createDataFrame(\n    [([&quot;BH_TCHH&quot;, &quot;BH_TCHF&quot;],)],\n    [&quot;column_list&quot;]\n)\n\n# Step 2: create df\ndfRaw = spark.createDataFrame(\n    [\n        (&quot;2024-01-01&quot;, &quot;id1&quot;, 10, 20),\n        (&quot;2024-01-02&quot;, &quot;id2&quot;, 30, 40)\n    ],\n    [&quot;ts&quot;, &quot;id&quot;, &quot;BH_TCHH&quot;, &quot;BH_TCHF&quot;]\n)\n\n# failure here\nwrong_column_name = bqDf.selectExpr(&quot;concat_ws(',', column_list)&quot;).collect()[0][0]\ntry:\n    display(\n        dfRaw.select(\n            &quot;ts&quot;,\n            &quot;id&quot;,\n            wrong_column_name\n        )\n    )\nexcept Exception as e:\n    print(&quot;ERROR:&quot;)\n    print(e)\n\n# this works\ncorrect_column_list = bqDf.select(&quot;column_list&quot;).collect()[0][0]\nfinalDf = dfRaw.select(\n    [&quot;ts&quot;, &quot;id&quot;] + correct_column_list\n)\ndisplay(finalDf)\n</code></pre>\n<p>Result :</p>\n<pre><code>ERROR:\n[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `BH_TCHH,BH_TCHF` cannot be resolved. Did you mean one of the following? [`BH_TCHF`, `BH_TCHH`, `id`, `ts`]. SQLSTATE: 42703;\n'Project [ts#13191, id#13192, 'BH_TCHH,BH_TCHF]\n+- Project [ts#13187 AS ts#13191, id#13188 AS id#13192, BH_TCHH#13189L AS BH_TCHH#13193L, BH_TCHF#13190L AS BH_TCHF#13194L]\n   +- LocalRelation [ts#13187, id#13188, BH_TCHH#13189L, BH_TCHF#13190L]\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n    at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:645)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:192)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:499)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:484)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1078)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1080)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:484)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:484)\n    at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n    at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:484)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n    at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n    at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n    at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n    at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n    at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n    at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n    at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n    at com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n    at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n    at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n    at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n    at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n    at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n    at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n    at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n    at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n    at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n    at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n    at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n    at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n    at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n    at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n    at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n    at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n    at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n    at org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n    at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n    at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n    at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n    at scala.util.Try$.apply(Try.scala:217)\n    at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n    at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n    at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n    at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n    at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n    at org.apache.spark.sql.classic.Dataset.&lt;init&gt;(Dataset.scala:404)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:114)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:211)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:79)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:71)\n    at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n    at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n    at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n    at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n    at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n    at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n    at org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:71)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:56)\n    at com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n    at org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:55)\n    at org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:116)\n    at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:870)\n    at org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n    at org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n    at org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n    at org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n    at org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n    at org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n    at org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n    at com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n    at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n    at com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n    at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n    at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n    at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n    at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n    at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n    at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n    at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n    at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n    at com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n    at com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n    at com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n    at com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n    at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n    at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n    at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n    at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n    at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n    at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n    at com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n    at com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n    at com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n    at com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n    at com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n    at org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n    at org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n    at org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n    at org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n    at org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n    at org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n    at org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n    at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n    at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n    at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n    at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n    at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.lang.Thread.run(Thread.java:840)\n</code></pre>\n<pre><code>ts  id  BH_TCHH BH_TCHF\n\n2024-01-01  id1 10  20\n\n2024-01-02  id2 30  40\n</code></pre>\n<p>scala case:</p>\n<p><code>yourdaframe.select(columnNames.map(col): _*)</code> will solve in case of scala</p>\n<p>Hope this solves your issue.</p>\n", "tags": ["scala", "apache-spark"]}
{"id": "a0130711-12d8-4b14-b473-39ebb6849198", "text": "Q: Setting Keystore file location using system properties which is inside resources directory of Spring boot A: <p>You need something like this:</p>\n\n<p>application.properties</p>\n\n<pre><code>server.port: 8443\nserver.ssl.key-store: classpath:${KEYSTORE:keystore.p12}\nserver.ssl.key-store-password: password\nserver.ssl.keyStoreType: PKCS12\nserver.ssl.keyAlias: tomcat\n</code></pre>\n\n<p>This will look for a system property KEYSTORE if not it will default to keystore.p12, so the application can be run like:</p>\n\n<pre><code>java -jar target/spring-boot-https-1.0.jar \n</code></pre>\n\n<p>or</p>\n\n<pre><code>java -DKEYSTORE=anotherKeystore.p12 -jar target/spring-boot-https-1.0.jar\n</code></pre>\n\n<p>That's all you need to do, if the keystore.p12 is in resources directory, to test</p>\n\n<pre><code>@RunWith(SpringRunner.class)\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\npublic class HelloControllerIT {\n\n    @LocalServerPort\n    private int port;\n\n    private RestTemplate template;\n\n    @Before\n    public void setUp() throws Exception {\n        createTemplateFromKeyStore(\"keystore.p12\");\n    }\n\n    @Test\n    public void getHello() throws Exception {\n        ResponseEntity&lt;String&gt; response = template.getForEntity(\"https://localhost:\" + port + \"/\", String.class);\n        assertThat(response.getBody(), equalTo(\"Greetings from Spring Boot!\"));\n    }\n\n    private void createTemplateFromKeyStore(String keyStoreName) {\n        try {\n            InputStream keyStoreInputStream = getClass().getResourceAsStream(keyStoreName);\n            KeyStore keyStore = KeyStore.getInstance(KeyStore.getDefaultType());\n            keyStore.load(keyStoreInputStream, null);\n\n            SSLContext sslContext = SSLContexts.custom()\n                    .loadKeyMaterial(keyStore, \"password\".toCharArray())\n                    .loadTrustMaterial(keyStore, new TrustAllStrategy()).build();\n\n            HttpClient httpClient = HttpClients.custom().setSSLContext(sslContext)\n                    .setSSLHostnameVerifier(NoopHostnameVerifier.INSTANCE).build();\n\n            HttpComponentsClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory();\n            requestFactory.setHttpClient(httpClient);\n\n            template = new RestTemplate(requestFactory);\n        } catch (IOException | GeneralSecurityException e) {\n            throw new RuntimeException(e);\n        }\n    }\n}\n</code></pre>\n\n<p>see <a href=\"https://github.com/gregclinker/spring-boot-https\" rel=\"nofollow noreferrer\">example project</a></p>\n\n<p>Having said all that you would be better off using <a href=\"https://www.mkyong.com/spring-boot/spring-boot-profile-based-properties-and-yaml-example/\" rel=\"nofollow noreferrer\">spring profiles</a> and have multiple application.properties</p>\n\n<pre><code>application-dev.properties\napplication-prod.properties\n</code></pre>\n\n<p>with different values in them and control from the command line</p>\n\n<pre><code>java -Dspring.profiles.active=dev -jar target/spring-boot-https-1.0.jar\n</code></pre>\n", "tags": ["java", "spring", "ssl", "spring-boot"]}
{"id": "f634f8aa-72ae-4be6-b6bc-1cd67c5c6c6f", "text": "Q: How to skip a command in batch A: <p>I would suggest perhaps doing it like this:</p>\n\n<pre class=\"lang-bat prettyprint-override\"><code>@Echo Off\nCD /D \"%ProgramFiles(x86)%\\Test123\" 2&gt;Nul||Exit /B\nTaskList|FindStr/I \"\\&lt;Test123.exe\\&gt;\"&gt;Nul&amp;&amp;(Start \"\" \"OtherTest123.exe\")||Start \"\" \"Test123.exe\"\n</code></pre>\n", "tags": ["windows", "batch-file", "cmd"]}
