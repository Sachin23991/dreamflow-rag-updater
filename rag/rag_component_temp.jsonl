{"id": "d2e7ff83-ba9a-43da-ae94-0b7f898991c0", "text": "Q: threejs get center of object A: <p>I like to use <code>.getCenter()</code> function</p>\n<pre><code>const box = new THREE.Box3().setFromObject(mesh);\nconst boxCenter = new THREE.Vector3();\nbox.getCenter(boxCenter);\n</code></pre>\n<p>Unfortunately, link for <code>Box3</code> does not work, but you can check <a href=\"https://threejs.org/docs/#api/en/math/Box2.getCenter\" rel=\"nofollow noreferrer\">Box2</a>.</p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\" data-babel-preset-react=\"false\" data-babel-preset-ts=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>let renderer, scene, camera;\n\ninit()\nanimate()\n\nfunction init() {\n  renderer = new THREE.WebGLRenderer()\n  renderer.setSize(window.innerWidth, window.innerHeight)\n  document.body.appendChild(renderer.domElement)\n\n  scene = new THREE.Scene()\n\n  camera = new THREE.PerspectiveCamera(\n    40,\n    window.innerWidth / window.innerHeight,\n    1,\n    100,\n  )\n  camera.position.set(0, 0, 5);\n\n  const geometry = new THREE.BoxGeometry(1, 1, 1)\n  const material = new THREE.MeshBasicMaterial({\n    color: 0x00ffff,\n    wireframe: true,\n  })\n\n  // mesh\n  const mesh = new THREE.Mesh(geometry, material)\n  scene.add(mesh)\n\n  const box = new THREE.Box3().setFromObject(mesh)\n  const boxCenter = new THREE.Vector3()\n  box.getCenter(boxCenter)\n\n  const dotGeometry = new THREE.BufferGeometry()\n  dotGeometry.setAttribute(\n    \"position\",\n    new THREE.BufferAttribute(new Float32Array(boxCenter), 3),\n  )\n  const dotMaterial = new THREE.PointsMaterial({\n    size: 0.5,\n    color: 0xff0000\n  })\n  const dot = new THREE.Points(dotGeometry, dotMaterial)\n  scene.add(dot)\n}\n\nfunction animate() {\n  requestAnimationFrame(animate)\n\n  renderer.render(scene, camera)\n}</code></pre>\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>* {\n  margin: 0;\n}</code></pre>\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/three@0.149.0/build/three.min.js\"&gt;&lt;/script&gt;</code></pre>\n</div>\n</div>\n</p>\n", "tags": ["3d", "three.js"]}
{"id": "f11c263c-df80-4987-8507-6e1b35da89ff", "text": "Q: AWS Glue PySpark job taking 4 hours to process small JSON files from S3 A: <p>I managed to solve this problem by <strong>bypassing Spark's native S3 file reading</strong> and using <strong>Boto3 to read files in parallel</strong>. This reduced my processing time from <strong>4 hours to 20 minutes</strong>.</p>\n<h3>Root Cause</h3>\n<p>Spark/PySpark has well-documented performance issues when reading many small files from S3. Each file read involves overhead (metadata operations, HTTP requests, etc.), and Spark's default S3 connector doesn't handle this efficiently at scale.</p>\n<h3>Solution: Boto3 + Spark Parallelization</h3>\n<p>Instead of using <code>spark.read.json()</code>, use Boto3 to list files and create a custom parallel loading function:</p>\n<p>python</p>\n<pre><code>import json\nimport boto3\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Initialize Spark\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\nbucket = 'your-bucket-name'\nprefix = 'your/prefix/path/'\n\n# Step 1: List all JSON files using Boto3 with pagination\ns3 = boto3.client('s3')\npaginator = s3.get_paginator('list_objects_v2')\npages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n\nfile_list = []\nfor page in pages:\n    if 'Contents' in page:\n        for obj in page['Contents']:\n            if obj['Key'].endswith('.json'):\n                file_list.append(obj['Key'])\n\nprint(f&quot;Found {len(file_list)} JSON files&quot;)\n\n# Step 2: Define custom loading function\n# IMPORTANT: Create S3 client INSIDE the function (not serializable otherwise)\ndef load_json_from_s3(key):\n    s3_client = boto3.client('s3')\n    try:\n        response = s3_client.get_object(Bucket=bucket, Key=key)\n        json_data = response['Body'].read()\n        return json.loads(json_data)\n    except Exception as e:\n        print(f&quot;Error loading {key}: {e}&quot;)\n        return None\n\n# Step 3: Parallelize file loading across Spark workers\noptimal_partitions = max(20, min(200, len(file_list) // 100))\ndata_rdd = sc.parallelize(file_list, numSlices=optimal_partitions).map(load_json_from_s3)\n\n# Step 4: Filter out None values and create DataFrame\ndata_rdd = data_rdd.filter(lambda x: x is not None)\n\n# Define your schema\nschema = StructType([\n    StructField(&quot;id&quot;, StringType(), True),\n    StructField(&quot;name&quot;, StringType(), True),\n    StructField(&quot;value&quot;, IntegerType(), True)\n    # Add your fields here\n])\n\nspark_df = spark.createDataFrame(data_rdd, schema=schema)\nspark_df.show()\n</code></pre>\n<h3>Key Points</h3>\n<ol>\n<li><p><strong>Boto3 S3 client inside the function</strong>: The S3 client must be created inside <code>load_json_from_s3()</code> because it's not serializable and cannot be broadcast to Spark workers.</p>\n</li>\n<li><p><strong>Optimal partitioning</strong>: Calculate partitions based on file count. Too few partitions = underutilized cluster; too many = excessive overhead.</p>\n</li>\n<li><p><strong>Pagination</strong>: Use <code>get_paginator('list_objects_v2')</code> for buckets with &gt;1000 objects.</p>\n</li>\n<li><p><strong>Error handling</strong>: Filter out <code>None</code> values from failed reads to prevent downstream errors.</p>\n</li>\n</ol>\n<h3>Performance Results</h3>\n<p><strong>Before:</strong></p>\n<ul>\n<li><p>Processing time: ~4 hours</p>\n</li>\n<li><p>Method: <code>spark.read.json(&quot;s3://...&quot;)</code></p>\n</li>\n</ul>\n<p><strong>After:</strong></p>\n<ul>\n<li><p>Processing time: ~20 minutes (<strong>12x faster</strong>)</p>\n</li>\n<li><p>Method: Boto3 parallel loading</p>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/\" rel=\"nofollow noreferrer\">AWS Big Data Blog: Best practices for successfully managing memory for Apache Spark applications on Amazon EMR</a></p>\n</li>\n<li><p><a href=\"https://www.databricks.com/blog/2022/05/11/how-to-deal-with-small-files-problem-in-data-lake.html\" rel=\"nofollow noreferrer\">Databricks: How to deal with small files problem in data lake</a></p>\n</li>\n</ul>\n", "tags": ["amazon-s3", "pyspark", "boto3", "etl", "aws-glue"]}
{"id": "8f033de0-8dfd-4b15-a773-84e6fac90f62", "text": "Q: Invalid settings: setting option &quot;vulncheck&quot;: invalid option &quot;Prompt&quot; for enum A: <p>Check this page: <a href=\"https://github.com/golang/vscode-go/wiki/settings\" rel=\"nofollow noreferrer\">https://github.com/golang/vscode-go/wiki/settings</a></p>\n<p>Press Ctrl+Shift+P, open User Settings (JSON), and if you want to enable vulnerability diagnostics for imported packages, add:</p>\n<pre><code>&quot;go.diagnostic.vulncheck&quot;: &quot;Imports&quot;\n</code></pre>\n<p>If you don\u2019t, use:</p>\n<pre><code>&quot;go.diagnostic.vulncheck&quot;: &quot;Off&quot;\n</code></pre>\n", "tags": ["go", "vscode-extensions"]}
{"id": "65c49b4c-be7d-41be-b630-0b710a74f8b6", "text": "Q: SqlDataAdapter.Fill() vs DataTable.Load() A: <p>Many years ago, I initially used DataTable &amp; Load.\nThen I found there was data missing.</p>\n<p>I believe I found a StackOverFlow post which suggested using DataAdapter &amp; Fill to fix this. Which I did and it worked.</p>\n<p>I have a vague memory of that some comment mentioned that Fill populates the Schema data correctly &amp; Load didn't. But I maybe mistaken. As I said, it was many years ago. But I have always used Fill ever since.</p>\n", "tags": ["c#", "sql", "sqldatareader", "sqldataadapter"]}
{"id": "546b8850-1e4f-42b6-82e0-0e2a4fdc60b5", "text": "Q: What is the idiomatic way to organize shared helper functions in Go projects? A: <p>I agree that it\u2019s not a \u201cstandard\u201d in the formal sense, and I didn\u2019t mean to present it as one.That said, even if no language has an official standard for project layout, most ecosystems still converge on a set of commonly accepted conventions over time \u2014 not because anyone is forced to follow them, but because they reduce friction and shared cognitive load.</p>\n<p>I see repositories like golang-standards/project-layout more as an attempt to document one such convention and the reasoning behind it, rather than to prescribe a mandatory structure. People are free to adopt parts of it, ignore it entirely, or evolve their own approach based on their needs.</p>\n", "tags": ["go"]}
